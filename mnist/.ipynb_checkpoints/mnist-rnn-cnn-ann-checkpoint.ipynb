{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets as datasets\n",
    "from torchvision import transforms as transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 2\n",
    "num_classes = 10\n",
    "input_size = 784\n",
    "sequence_length=28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to dataset/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a165ad6c267491bbdfaef0aa8b0ce32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/MNIST/raw/train-images-idx3-ubyte.gz to dataset/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to dataset/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e26d480339542fbb1c89502b2f78ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/MNIST/raw/train-labels-idx1-ubyte.gz to dataset/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to dataset/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd27688eb39944df8bf1ef14abc74fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/MNIST/raw/t10k-images-idx3-ubyte.gz to dataset/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea6f09da9bd446ba0f810d395159483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz to dataset/MNIST/raw\n",
      "Processing...\n",
      "Done!\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikhil/miniconda3/envs/myenv/lib/python3.7/site-packages/torchvision/datasets/mnist.py:469: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "train_data = datasets.MNIST(root='dataset/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = datasets.MNIST(root='dataset/', train=False, transform=transforms.ToTensor(), download=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(ANN, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, 1024)\n",
    "        self.linear2 = nn.Linear(1024, 512)\n",
    "        self.linear3 = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.linear2(x))\n",
    "        #print(x.shape)\n",
    "        x = self.linear3(x)\n",
    "        #print(x.shape)\n",
    "        return x\n",
    "    \n",
    "class RNN(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size = input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size*sequence_length, num_classes)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        out, _ = self.rnn(x,h0)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ANN(input_size, num_classes).to(device)\n",
    "#model = RNN(input_size = 28, hidden_size=256, num_layers = 2, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random_data = torch.rand(32 ,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output = model(random_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:04<00:00, 198.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/2 , loss:2.312607526779175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 16/157 [00:00<00:00, 155.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 phase:train loss:230.127674369812 Accu:12.925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:00<00:00, 240.25it/s]\n",
      "  2%|▏         | 22/938 [00:00<00:04, 218.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/2 , loss:2.3005924224853516\n",
      "Epoch:0 phase:valid loss:230.1007992553711 Accu:12.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:04<00:00, 229.83it/s]\n",
      " 15%|█▌        | 24/157 [00:00<00:00, 234.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 , loss:2.312126398086548\n",
      "Epoch:1 phase:train loss:230.12767345428466 Accu:12.925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:00<00:00, 242.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 , loss:2.30816912651062\n",
      "Epoch:1 phase:valid loss:230.10079692840577 Accu:12.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "                \n",
    "num_samples = 0 \n",
    "total_corrects = 0\n",
    "total_loss = 0.0\n",
    "total_acc = 0\n",
    "\n",
    "model = ANN(input_size, num_classes).to(device)\n",
    "\n",
    "training_dict = {'train':train_loader, \"valid\":test_loader}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "\n",
    "    for phase in [\"train\", \"valid\"]:\n",
    "        if phase == \"train\":\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        num_samples = 0 \n",
    "        total_corrects = 0\n",
    "        total_loss =0.0\n",
    "        total_acc = 0\n",
    "        \n",
    "        loop = tqdm(enumerate(training_dict[phase]),total=len(training_dict[phase]))\n",
    "        \n",
    "\n",
    "        for btch, (inputs, outputs) in loop:\n",
    "            \n",
    "            #print(inputs.size())\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = outputs.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.set_grad_enabled(phase == \"train\"):\n",
    "                \n",
    "                inputs = inputs.reshape(inputs.shape[0], -1)\n",
    "                pred = model(inputs)\n",
    "                loss = criterion(pred, outputs)\n",
    "                _,predictions = pred.max(1)\n",
    "                #print(pred.max(1))\n",
    "                \n",
    "                if phase == \"train\":\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                #print(predictions.shape, num_samples)\n",
    "                num_samples += predictions.size(0)\n",
    "\n",
    "                #print(pred, outputs)\n",
    "                total_corrects += torch.sum(predictions==outputs)\n",
    "                #print(predictions, outputs, predictions==outputs)\n",
    "                total_loss += loss.item() * inputs.size(0)\n",
    "                #print(num_samples, total_corrects, pred.shape)\n",
    "        print('Epoch {}/{} , loss:{}'.format(epoch,num_epochs,loss.item()))\n",
    "        \n",
    "        #print(total_corrects, num_samples)\n",
    "        total_acc = (total_corrects.double() * 100) / num_samples\n",
    "        total_loss = (total_loss * 100) / num_samples\n",
    "        print('Epoch:{} phase:{} loss:{} Accu:{}'.format(epoch, phase, total_loss, total_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training RNN\n",
    "model = RNN(input_size = 28, hidden_size=256, num_layers = 2, num_classes=10)\n",
    "\n",
    "        \n",
    "                \n",
    "num_samples = 0 \n",
    "total_corrects = 0\n",
    "total_loss = 0.0\n",
    "total_acc = 0\n",
    "\n",
    "training_dict = {'train':train_loader, \"valid\":test_loader}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for phase in [\"train\", \"valid\"]:\n",
    "        if phase == \"train\":\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "        \n",
    "                \n",
    "        num_samples = 0 \n",
    "        total_corrects = 0\n",
    "        total_loss = 0.0\n",
    "\n",
    "    \n",
    "        loop = tqdm(enumerate(training_dict[phase]),total=len(training_dict[phase]))\n",
    "        \n",
    "\n",
    "        for btch, (inputs, outputs) in loop:\n",
    "            \n",
    "            #print(inputs.size())\n",
    "            inputs = inputs.to(device).squeeze(1)\n",
    "            outputs = outputs.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "                            \n",
    "            with torch.set_grad_enabled(phase == \"train\"):\n",
    "                \n",
    "                pred = model(inputs)\n",
    "                loss = criterion(pred, outputs)\n",
    "                _,pred = pred.max(1)\n",
    "                #print(pred)\n",
    "                \n",
    "                if phase == \"train\":\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                num_samples += pred.size(0)\n",
    "                #print(pred, outputs)\n",
    "                total_corrects += (pred==outputs).sum()\n",
    "                total_loss += loss.item() * inputs.size(0)\n",
    "                #print(num_samples, total_corrects, pred.shape)\n",
    "        \n",
    "        print('Epoch {}/{} , loss:{}'.format(epoch,num_epochs,loss.item()))\n",
    "        \n",
    "        #print(total_corrects, num_samples)\n",
    "        total_acc = (total_corrects.double() * 100) / num_samples\n",
    "        total_loss = (total_loss * 100) / num_samples\n",
    "        print('Epoch:{} phase:{} loss:{} Accu:{}'.format(epoch, phase, total_loss, total_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
